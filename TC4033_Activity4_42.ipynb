{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aarmentamna/machine_learning_advance/blob/main/TC4033_Activity4_42.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uFYXOadkCwN2",
      "metadata": {
        "id": "uFYXOadkCwN2"
      },
      "source": [
        "## **Maestría en Inteligencia Artificial Aplicada**\n",
        "### **Curso: ADVANCED MACHINE LEARNING METHODS**\n",
        "## Tecnológico de Monterrey\n",
        "### Dr. José Antonio Cantoral Ceballos\n",
        "\n",
        "## Activity Week 9\n",
        "### **Text Generator.**\n",
        "\n",
        "*TEAM MEMBERS:*\n",
        "\n",
        "*   Roberto Romero Vielma - A00822314\n",
        "*   José Javier Granados Hernández - A00556717\n",
        "*   Aquiles Yonatan Armenta Hernandez - A01793252\n",
        "*   Alan Avelino Fernández Juárez - A00989308"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "037e89c8",
      "metadata": {
        "id": "037e89c8"
      },
      "source": [
        "## TC 5033\n",
        "### Text Generation\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Activity 4: Building a Simple LSTM Text Generator using WikiText-2\n",
        "<br>\n",
        "\n",
        "- Objective:\n",
        "    - Gain a fundamental understanding of Long Short-Term Memory (LSTM) networks.\n",
        "    - Develop hands-on experience with sequence data processing and text generation in PyTorch. Given the simplicity of the model, amount of data, and computer resources, the text you generate will not replace ChatGPT, and results must likely will not make a lot of sense. Its only purpose is academic and to understand the text generation using RNNs.\n",
        "    - Enhance code comprehension and documentation skills by commenting on provided starter code.\n",
        "    \n",
        "<br>\n",
        "\n",
        "- Instructions:\n",
        "    - Code Understanding: Begin by thoroughly reading and understanding the code. Comment each section/block of the provided code to demonstrate your understanding. For this, you are encouraged to add cells with experiments to improve your understanding\n",
        "\n",
        "    - Model Overview: The starter code includes an LSTM model setup for sequence data processing. Familiarize yourself with the model architecture and its components. Once you are familiar with the provided model, feel free to change the model to experiment.\n",
        "\n",
        "    - Training Function: Implement a function to train the LSTM model on the WikiText-2 dataset. This function should feed the training data into the model and perform backpropagation.\n",
        "\n",
        "    - Text Generation Function: Create a function that accepts starting text (seed text) and a specified total number of words to generate. The function should use the trained model to generate a continuation of the input text.\n",
        "\n",
        "    - Code Commenting: Ensure that all the provided starter code is well-commented. Explain the purpose and functionality of each section, indicating your understanding.\n",
        "\n",
        "    - Submission: Submit your Jupyter Notebook with all sections completed and commented. Include a markdown cell with the full names of all contributing team members at the beginning of the notebook.\n",
        "    \n",
        "<br>\n",
        "\n",
        "- Evaluation Criteria:\n",
        "    - Code Commenting (60%): The clarity, accuracy, and thoroughness of comments explaining the provided code. You are suggested to use markdown cells for your explanations.\n",
        "\n",
        "    - Training Function Implementation (20%): The correct implementation of the training function, which should effectively train the model.\n",
        "\n",
        "    - Text Generation Functionality (10%): A working function is provided in comments. You are free to use it as long as you make sure to uderstand it, you may as well improve it as you see fit. The minimum expected is to provide comments for the given function.\n",
        "\n",
        "    - Conclusions (10%): Provide some final remarks specifying the differences you notice between this model and the one used  for classification tasks. Also comment on changes you made to the model, hyperparameters, and any other information you consider relevant. Also, please provide 3 examples of generated texts.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ypC8MQo7kuR",
      "metadata": {
        "id": "3ypC8MQo7kuR"
      },
      "source": [
        "##Install mising libraries\n",
        "\n",
        "To run the code successfully, you need to install the PyTorch, torchtext and portalocker libraries. The following code uses pip to install these libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zBBbIoHGCwN6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBBbIoHGCwN6",
        "outputId": "4da0a055-0b55-4328-96dc-c331d9cf99cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\n",
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.1.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.23.5)\n",
            "Requirement already satisfied: torchdata==0.7.0 in /usr/local/lib/python3.10/dist-packages (from torchtext) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (2.1.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.0->torchtext) (2.0.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchtext) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchtext) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# Install the torch library\n",
        "!pip install torch\n",
        "\n",
        "# Install the torchtext library\n",
        "!pip install torchtext\n",
        "\n",
        "# Install portalocker with version greater than or equal to 2.0.0\n",
        "!pip install portalocker>=2.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3eb4b117",
      "metadata": {
        "id": "3eb4b117"
      },
      "outputs": [],
      "source": [
        "# Import torch\n",
        "# torch: A deep learning library for creating and training neural networks.\n",
        "import torch\n",
        "\n",
        "# Import torchtext\n",
        "# torchtext: A library for text processing, including dataset loading and tokenization utilities.\n",
        "import torchtext\n",
        "\n",
        "# Import specific classes/modules from torchtext\n",
        "# WikiText2: Dataset class for loading the WikiText-2 dataset.\n",
        "from torchtext.datasets import WikiText2\n",
        "\n",
        "# Import DataLoader, TensorDataset, and random_split from torch.utils.data\n",
        "# DataLoader: Iterable over a dataset, allowing batch loading of data.\n",
        "# TensorDataset: Dataset wrapping tensors, useful for creating input-output pairs.\n",
        "# random_split: Splits a dataset into random train and validation subsets.\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "# Import get_tokenizer from torchtext.data.utils\n",
        "# get_tokenizer: Function to get a tokenizer for text processing.\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "# Import build_vocab_from_iterator from torchtext.vocab\n",
        "# build_vocab_from_iterator: Function for building a vocabulary from an iterator of tokenized texts.\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "# Import to_map_style_dataset from torchtext.data.functional\n",
        "# to_map_style_dataset: Function for converting a dataset to the map-style dataset.\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "\n",
        "# Import nn and F (functional) from torch\n",
        "# nn (neural network): Basic building blocks for creating and training neural networks.\n",
        "# F (functional): Contains functions that operate on tensors and are used in building neural network layers.\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Import optim from torch\n",
        "# optim: Contains various optimization algorithms such as SGD, Adam, etc., for training neural networks.\n",
        "import torch.optim as optim\n",
        "\n",
        "# Import tqdm\n",
        "# tqdm: A library for creating progress bars to visualize the progress of loops or computations.\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Import numpy as np\n",
        "# np (numpy): A powerful library for numerical operations in Python, providing support for large, multi-dimensional arrays and matrices.\n",
        "import numpy as np\n",
        "\n",
        "# Import random\n",
        "# random: Module for generating random numbers and performing random operations.\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vjDhjjHJ7pic",
      "metadata": {
        "id": "vjDhjjHJ7pic"
      },
      "source": [
        "This code imports various libraries and modules related to PyTorch, data handling, and neural networks. Here's a summary of the imported libraries:\n",
        "\n",
        "1. PyTorch Libraries:\n",
        "  - import torch: Core PyTorch library.\n",
        "  - import torchtext: Library for text processing with PyTorch.\n",
        "  - from torchtext.datasets import WikiText2: Importing the WikiText2 dataset from the torchtext.datasets module.\n",
        "\n",
        "2. Dataloader Library:\n",
        "  - from torch.utils.data import DataLoader, TensorDataset: Importing DataLoader and TensorDataset classes for efficient data loading.\n",
        "\n",
        "3. Libraries to Prepare the Data:\n",
        "  - from torchtext.data.utils import get_tokenizer: Importing a tokenizer for text processing.\n",
        "  - from torchtext.vocab import build_vocab_from_iterator: Importing a function to build vocabulary from iterators.\n",
        "  - from torchtext.data.functional import to_map_style_dataset: Importing a function to convert a dataset to a map-style dataset.\n",
        "\n",
        "4. Neural Layers:\n",
        "  - from torch import nn: Importing the neural network module from PyTorch.\n",
        "  - from torch.nn import functional as F: Importing the functional module from PyTorch, often used for activation functions.\n",
        "  - import torch.optim as optim: Importing the PyTorch module for optimization algorithms.\n",
        "\n",
        "5. Other Utilities:\n",
        "  - from tqdm import tqdm: Importing the tqdm library for creating progress bars during iterations.\n",
        "  - import numpy as np: Importing NumPy for numerical operations.\n",
        "  - import random: Importing the random module for generating random numbers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "K4x5i37ezl69",
      "metadata": {
        "id": "K4x5i37ezl69"
      },
      "source": [
        "##Define M1 Mac GPU Speed or CUDA if available\n",
        "\n",
        "This code checks for the availability of CUDA-enabled GPU (Graphics Processing Unit) for PyTorch and sets the device accordingly. Here's a step-by-step explanation of the code:\n",
        "\n",
        "1. It uses `torch.cuda.is_available()` to check if a CUDA-enabled GPU is available.\n",
        "\n",
        "2. If a GPU is available (torch.cuda.is_available() returns True), it sets the device to 'cuda' using torch.device('cuda').\n",
        "\n",
        "3. If a GPU is not available, it attempts to set the device to 'mps' (Multi-Process Service), which is a feature provided by some CUDA-enabled GPUs for improved multi-GPU performance. This is done inside a try-except block, which means if 'mps' is not supported or available, it will fall back to using the CPU.\n",
        "\n",
        "4. If neither 'cuda' nor 'mps' is available, it sets the device to 'cpu' using torch.device('cpu').\n",
        "\n",
        "5. Finally, it prints the selected device to the console, indicating whether it's 'cuda' (GPU), 'mps', or 'cpu'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d8ff971",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d8ff971",
        "outputId": "9be2454d-524e-44f4-9944-2ef6cdfa186c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Check if CUDA (GPU) is available\n",
        "if torch.cuda.is_available():\n",
        "    # If CUDA is available, set the device to CUDA\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    try:\n",
        "        # If CUDA is not available, try using MPS (Multi-Process Service)\n",
        "        device = torch.device('mps')\n",
        "    except:\n",
        "        # If both CUDA and MPS are not available, fall back to CPU\n",
        "        device = torch.device('cpu')\n",
        "\n",
        "# Print the selected device\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xxEpuLit8b-N",
      "metadata": {
        "id": "xxEpuLit8b-N"
      },
      "source": [
        "Next line of code is loading the WikiText2 dataset and assigning it to three variables: train_dataset, val_dataset, and test_dataset. Here's a step-by-step explanation of the code:\n",
        "\n",
        "1. Dataset Loading: The `WikiText2()` function is used to load the WikiText2 dataset. This dataset is commonly used in natural language processing tasks, particularly for language modeling.\n",
        "\n",
        "2. Data Splitting: The WikiText2 dataset is split into three subsets:\n",
        "  - `train_dataset`: This subset is typically used for training a machine learning model. It contains examples that the model will learn from.\n",
        "  - `val_dataset`: This subset is used for validation during the training process. It helps monitor the model's performance on data it has not seen during training and can be used for hyperparameter tuning.\n",
        "  - `test_dataset`: This subset is reserved for evaluating the final performance of the trained model. It is used to assess how well the model generalizes to new, unseen data.\n",
        "\n",
        "3. Variable Assignment: The three datasets (train, validation, and test) are assigned to the variables train_dataset, val_dataset, and test_dataset, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3288ce5",
      "metadata": {
        "id": "f3288ce5"
      },
      "outputs": [],
      "source": [
        "# Load WikiText2 dataset and split it into training, validation, and test sets\n",
        "train_dataset, val_dataset, test_dataset = WikiText2()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mgRvueW68xBR",
      "metadata": {
        "id": "mgRvueW68xBR"
      },
      "source": [
        "Next code is defining a tokenization process using the get_tokenizer function from the torchtext.data.utils module. Here's a step-by-step explanation of the code:\n",
        "\n",
        "1. Tokenization Setup:\n",
        "\n",
        "  - `tokeniser = get_tokenizer('basic_english')`: This line sets up a tokenizer named tokeniser using the 'basic_english' preset provided by get_tokenizer. This particular tokenizer is designed for basic English text processing.\n",
        "\n",
        "2. Tokenization Function:\n",
        "\n",
        "  - `def yield_tokens(data):`: This line defines a function named yield_tokens that takes a dataset (data) as input.\n",
        "  - `for text in data:`: It iterates over each text in the dataset.\n",
        "  - `yield tokeniser(text)`: For each text, it uses the tokeniser to tokenize the text and yields the resulting tokens. The yield keyword is used here to create a generator function, allowing tokens to be generated one at a time rather than storing all tokens in memory at once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc4c7dbd",
      "metadata": {
        "id": "fc4c7dbd"
      },
      "outputs": [],
      "source": [
        "# Define a tokenizer using the 'basic_english' tokenization method from torchtext\n",
        "tokeniser = get_tokenizer('basic_english')\n",
        "\n",
        "# Define a function that yields tokens for a given dataset\n",
        "def yield_tokens(data):\n",
        "    # Iterate through each text in the dataset\n",
        "    for text in data:\n",
        "        # Tokenize the text using the defined tokenizer\n",
        "        yield tokeniser(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wTieXgEq-SzA",
      "metadata": {
        "id": "wTieXgEq-SzA"
      },
      "source": [
        "Next code is focused on building a vocabulary from a training dataset for natural language processing tasks. Here's a step-by-step explanation of the code:\n",
        "\n",
        "1. Build Vocabulary:\n",
        "  - `vocab = build_vocab_from_iterator(yield_tokens(train_dataset), specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])`:\n",
        "    - `yield_tokens(train_dataset)`: This generates tokens from the training dataset using the yield_tokens function. It is passed as an iterator to build_vocab_from_iterator.\n",
        "    - `specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]`: These are special tokens that are added to the vocabulary. For example, `<unk>` represents unknown words, `<pad>` represents padding, `<bos>` is the beginning-of-sequence token, and `<eos>` is the end-of-sequence token.\n",
        "    - `vocab = ...`: The result is a vocabulary (`vocab`) constructed from the tokens in the training dataset.\n",
        "\n",
        "2. Set Default Index for Unknown Tokens:\n",
        "  - `vocab.set_default_index(vocab[\"<unk>\"])`:\n",
        "    -  `vocab[\"<unk>\"]`: Retrieves the index of the `<unk>` (unknown) token in the vocabulary.\n",
        "    - `vocab.set_default_index(...)`: Sets the default index of the vocabulary to the index of the `<unk>` token. This means that if an out-of-vocabulary word is encountered during later processing, it will be represented by the index of the `<unk>` token.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c2cb068",
      "metadata": {
        "id": "2c2cb068"
      },
      "outputs": [],
      "source": [
        "# Build the vocabulary from the tokenized training dataset\n",
        "# yield_tokens(train_dataset): Tokenize the training dataset using the previously defined tokenizer\n",
        "# specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]: Define special tokens for unknown, padding, beginning of sequence, and end of sequence\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_dataset), specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
        "\n",
        "# Set the default index for the vocabulary to the index of the unknown token (\"<unk>\")\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6emV5kpDAMnt",
      "metadata": {
        "id": "6emV5kpDAMnt"
      },
      "source": [
        "Next code is part of the data processing pipeline for preparing sequences of text data for training a model. Here's a step-by-step explanation of the code:\n",
        "\n",
        "1. Set Sequence Length:\n",
        "  - `seq_length = 50`: This line defines the sequence length, indicating the number of tokens in each sequence. In this case, it is set to 50.\n",
        "\n",
        "2. Data Processing Function:\n",
        "  - `def data_process(raw_text_iter, seq_length=50)`: This function takes an iterator (`raw_text_iter`) containing raw text data and an optional parameter for the sequence length.\n",
        "  - `data = [torch.tensor(vocab(tokeniser(item)), dtype=torch.long) for item in raw_text_iter]`: It tokenizes each item in the raw text iterator using the `vocab` and `tokeniser` functions, then converts the resulting tokens into PyTorch tensors of type `long`.\n",
        "  - `data = torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))`: It concatenates the tensors into a single tensor along the first dimension (using `torch.cat`). The `filter` function is used to remove any empty tensors.\n",
        "  - The function returns a tuple containing two tensors:\n",
        "    - `data[:-(data.size(0) % seq_length)].view(-1, seq_length)`: This tensor represents the input data. It is reshaped into sequences of length `seq_length`. The part before the last sequence that doesn't fit evenly into s`eq_length` is discarded.\n",
        "    - `data[1:-(data.size(0) % seq_length - 1)].view(-1, seq_length)`: This tensor represents the target data, shifted by one position compared to the input data. It is also reshaped into sequences of length `seq_length`.\n",
        "\n",
        "4. Create Tensors for Training, Validation, and Test Sets:\n",
        "  - `x_train, y_train = data_process(train_dataset, seq_length)`: Calls the `data_process` function on the training dataset, generating input (`x_train`) and target (`y_train`) tensors.\n",
        "  - Similar lines create tensors for the validation (`x_val, y_val`) and test (`x_test, y_test`) datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "134b832b",
      "metadata": {
        "id": "134b832b"
      },
      "outputs": [],
      "source": [
        "# Define the sequence length\n",
        "seq_length = 50\n",
        "\n",
        "# Define a function for data processing\n",
        "def data_process(raw_text_iter, seq_length=50):\n",
        "    # Tokenize each item in the raw text iterator using the vocabulary and convert them to torch tensors\n",
        "    data = [torch.tensor(vocab(tokeniser(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "\n",
        "    # Concatenate non-empty tensors and remove empty tensors\n",
        "    data = torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
        "\n",
        "    # Remove the remainder to ensure even sequence lengths and reshape the tensors\n",
        "    return (\n",
        "        data[:-(data.size(0) % seq_length)].view(-1, seq_length),\n",
        "        data[1:-(data.size(0) % seq_length - 1)].view(-1, seq_length)\n",
        "    )\n",
        "\n",
        "# Process the training dataset to create tensors\n",
        "x_train, y_train = data_process(train_dataset, seq_length)\n",
        "\n",
        "# Process the validation dataset to create tensors\n",
        "x_val, y_val = data_process(val_dataset, seq_length)\n",
        "\n",
        "# Process the test dataset to create tensors\n",
        "x_test, y_test = data_process(test_dataset, seq_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0RqaKrJBBjel",
      "metadata": {
        "id": "0RqaKrJBBjel"
      },
      "source": [
        "Next code is creating PyTorch TensorDataset instances for the training, validation, and test sets. Here's a summary:\n",
        "\n",
        "- Create TensorDataset Instances:\n",
        "  - `train_dataset = TensorDataset(x_train, y_train)`: This line creates a `TensorDataset` instance for the training set. It takes two tensors (`x_train` and `y_train`) representing input and target sequences, respectively.\n",
        "  - `val_dataset = TensorDataset(x_val, y_val)`: Similarly, a `TensorDataset` instance is created for the validation set.\n",
        "  - `test_dataset = TensorDataset(x_test, y_test)`: Likewise, a `TensorDataset` instance is created for the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b54c04d",
      "metadata": {
        "id": "4b54c04d"
      },
      "outputs": [],
      "source": [
        "# Create a PyTorch TensorDataset for the training set\n",
        "train_dataset = TensorDataset(x_train, y_train)\n",
        "\n",
        "# Create a PyTorch TensorDataset for the validation set\n",
        "val_dataset = TensorDataset(x_val, y_val)\n",
        "\n",
        "# Create a PyTorch TensorDataset for the test set\n",
        "test_dataset = TensorDataset(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ckQvpKmCAyF",
      "metadata": {
        "id": "0ckQvpKmCAyF"
      },
      "source": [
        "Next code sets up PyTorch DataLoader instances for the training, validation, and test sets, enabling efficient loading of data in batches during model training and evaluation. Here's a summary:\n",
        "\n",
        "1. Define Batch Size:\n",
        "   - `batch_size = 64`: This line sets the batch size, which is the number of samples processed in one iteration during training.\n",
        "\n",
        "2. Create `DataLoader` Instances:\n",
        "  - `train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)`: This line creates a `DataLoader` for the training set (`train_dataset`). It specifies the batch size, enables shuffling of the data before each epoch (`huffle=True`), and drops the last incomplete batch if the dataset size is not divisible by the batch size (`drop_last=True`).\n",
        "  - `val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=True)`: Similarly, a `DataLoader` is created for the validation set (`val_dataset`).\n",
        "  - `test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)`: Likewise, a `DataLoader` is created for the test set (`test_dataset`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4d400fb",
      "metadata": {
        "id": "f4d400fb"
      },
      "outputs": [],
      "source": [
        "# Define the batch size\n",
        "batch_size = 64\n",
        "\n",
        "# Create a DataLoader for the training set\n",
        "# DataLoader: Provides an iterable over a dataset, allowing batch loading of data.\n",
        "# batch_size: Number of samples per batch to load.\n",
        "# shuffle: Set to True to have the data reshuffled at every epoch.\n",
        "# drop_last: Set to True to drop the last incomplete batch if the dataset size is not divisible by the batch size.\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "# Create a DataLoader for the validation set\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "# Create a DataLoader for the test set\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YIMTWAEHCsjr",
      "metadata": {
        "id": "YIMTWAEHCsjr"
      },
      "source": [
        "Next code defines an LSTM (Long Short-Term Memory) model using PyTorch's nn.Module interface. Here's a step-by-step summary:\n",
        "\n",
        "1. Define the LSTM Model Class:\n",
        "  - `class LSTMModel(nn.Module):`: Defines a new class `LSTMModel` that inherits from `nn.Module`, indicating that it is a PyTorch model.\n",
        "  - `def __init__(self, vocab_size, embed_size, hidden_size, num_layers):`: Initializes the model with specified hyperparameters.\n",
        "    - `self.embeddings = nn.Embedding(vocab_size, embed_size)`: Defines an embedding layer with vocabulary size `vocab_size` and embedding size `embed_size`.\n",
        "    - `self.hidden_size = hidden_size`: Stores the hidden size as an attribute.\n",
        "    - `self.num_layers = num_layers`: Stores the number of LSTM layers as an attribute.\n",
        "    - `self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)`: Defines an LSTM layer with input size `embed_size`, hidden size `hidden_size`, `num_layers` LSTM layers, and `batch_first=True` indicating that the input and output tensors are provided as (batch, seq, feature).\n",
        "    - `self.fc = nn.Linear(hidden_size, vocab_size)`: Defines a fully connected layer with input size `hidden_size` and output size `vocab_size`, mapping LSTM output to vocabulary size.\n",
        "\n",
        "  - `def forward(self, text, hidden):`: Defines the forward pass of the model.\n",
        "    - `embeddings = self.embeddings(text)`: Applies the embedding layer to the input text.\n",
        "    - `output, hidden = self.lstm(embeddings, hidden)`: Applies the LSTM layer to the embeddings, producing output and hidden states.\n",
        "    - `decoded = self.fc(output)`: Applies the fully connected layer to the LSTM output.\n",
        "    - `return decoded, hidden`: Returns the decoded output and the hidden state.\n",
        "\n",
        "  - `def init_hidden(self, batch_size):`: Initializes the hidden state of the LSTM.\n",
        "    - `return (torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device), torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device))`: Returns a tuple containing two tensors of zeros, representing the initial hidden state and cell state of the LSTM.\n",
        "\n",
        "2. Set Model Hyperparameters:\n",
        "  - `vocab_size = len(vocab)`: Determines the vocabulary size based on the size of the vocabulary (`vocab`).\n",
        "  - `emb_size = 100`: Sets the embedding size to 100.\n",
        "  - `neurons = 128`: Sets the number of neurons (hidden size) to 128.\n",
        "  - `num_layers = 1`: Sets the number of LSTM layers to 1.\n",
        "\n",
        "3. Instantiate the Model:\n",
        "  - `model = LSTMModel(vocab_size, emb_size, neurons, num_layers)`: Creates an instance of the `LSTMModel` with the specified hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59c63b01",
      "metadata": {
        "id": "59c63b01"
      },
      "outputs": [],
      "source": [
        "# Define a custom LSTM model class\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "        '''\n",
        "        Initialize the LSTMModel.\n",
        "\n",
        "        Parameters:\n",
        "            vocab_size (int): The size of the vocabulary.\n",
        "            embed_size (int): The size of the word embeddings.\n",
        "            hidden_size (int): The number of features in the hidden state of the LSTM.\n",
        "            num_layers (int): The number of LSTM layers.\n",
        "        '''\n",
        "        # Call the constructor of the parent class (nn.Module)\n",
        "        super(LSTMModel, self).__init__()\n",
        "\n",
        "        # Embedding layer: Converts token indices to dense vectors of fixed size\n",
        "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
        "\n",
        "        # LSTM layer: Long Short-Term Memory network\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "        # Fully connected layer: Linear transformation to output vocab_size predictions\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        # Store hidden_size for initializing hidden state\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Store num_layers for controlling the LSTM layers\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def forward(self, text, hidden):\n",
        "        # Embedding layer input\n",
        "        embeddings = self.embeddings(text)\n",
        "\n",
        "        # LSTM layer forward pass\n",
        "        output, hidden = self.lstm(embeddings, hidden)\n",
        "\n",
        "        # Fully connected layer for predictions\n",
        "        decoded = self.fc(output)\n",
        "\n",
        "        # Return the output and hidden state\n",
        "        return decoded, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        # Return a tuple containing two tensors representing the initial hidden state for the LSTM.\n",
        "\n",
        "        # The first tensor (h) is the initial hidden state with dimensions (num_layers, batch_size, hidden_size).\n",
        "        # The second tensor is the initial cell state (c) with the same dimensions.\n",
        "\n",
        "        # Initialize both tensors with zeros and move them to the specified device (GPU or CPU).\n",
        "        return (\n",
        "    torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device),\n",
        "    torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
        "        )\n",
        "\n",
        "# Set model hyperparameters\n",
        "vocab_size = len(vocab)  # Vocabulary size\n",
        "emb_size = 100  # Embedding size\n",
        "neurons = 128  # Number of neurons in the LSTM layer\n",
        "num_layers = 1  # Number of LSTM layers\n",
        "\n",
        "# Create an instance of the LSTMModel\n",
        "model = LSTMModel(vocab_size, emb_size, neurons, num_layers)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6sU3PrJuE4Ev",
      "metadata": {
        "id": "6sU3PrJuE4Ev"
      },
      "source": [
        "Next code defines a training function for a given model using PyTorch. Here's a step-by-step summary:\n",
        "\n",
        "1. Define the Training Function:\n",
        "  - `def train(model, epochs, optimizer, criterion):`: Defines a training function that takes a model, the number of epochs, an optimizer, and a loss criterion as inputs.\n",
        "\n",
        "2. Move Model to Device and Set to Train Mode:\n",
        "  - `model = model.to(device=device)`: Moves the model to the specified device (CPU or GPU).\n",
        "  - `model.train()`: Sets the model in training mode, enabling features like dropout.\n",
        "\n",
        "3. Training Loop Over Epochs:\n",
        "  - `for epoch in range(epochs):`: Iterates over the specified number of epochs.\n",
        "\n",
        "4. Initialize Metrics:\n",
        "  - `total_loss = 0.0`: Initializes the total loss for the epoch.\n",
        "  - `correct_predictions = 0`: Initializes the count of correct predictions.\n",
        "  - `total_samples = 0`: Initializes the total number of samples processed.\n",
        "\n",
        "5. Training Loop Over Batches:\n",
        "  - `for i, (data, targets) in enumerate(train_loader):`: Iterates over batches in the training data.\n",
        "  - `optimizer.zero_grad()`: Zeroes the gradients to prevent accumulation.\n",
        "\n",
        "6. Move Data to Device and Initialize Hidden State:\n",
        "  - `data, targets = data.to(device), targets.to(device)`: Moves the input data and targets to the specified device.\n",
        "  - `hidden = model.init_hidden(data.size(0))`: Initializes the hidden state for the LSTM model.\n",
        "\n",
        "7. Forward Pass:\n",
        "  - `outputs, hidden = model(data, hidden)`: Performs a forward pass through the model.\n",
        "\n",
        "8. Calculate Loss and Update Metrics:\n",
        "  - `loss = criterion(outputs.view(-1, len(vocab)), targets.view(-1))`: Calculates the loss using the specified criterion.\n",
        "  - `total_loss += loss.item()`: Accumulates the total loss for the epoch.\n",
        "  - `_, predicted = torch.max(outputs, 2)`: Obtains predictions by selecting the index with the maximum value along the third dimension.\n",
        "  - `correct_predictions += (predicted == targets).sum().item()`: Counts the number of correct predictions.\n",
        "  - `total_samples += targets.numel()`: Updates the total number of processed samples.\n",
        "\n",
        "9. Backward Pass and Optimization:\n",
        "  - `loss.backward()`: Performs a backward pass to compute gradients.\n",
        "  - `optimizer.step()`: Updates the model parameters using the optimizer.\n",
        "\n",
        "10. Compute Metrics for the Epoch and Print:\n",
        "  - `loss = total_loss / len(train_loader)`: Computes the average loss for the epoch.\n",
        "  - `accuracy = correct_predictions / total_samples`: Computes the accuracy for the epoch.\n",
        "  - `print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss:.4f}, Accuracy: {accuracy * 100:.2f}%')`: Prints the epoch, loss, and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "215eabb9",
      "metadata": {
        "id": "215eabb9"
      },
      "outputs": [],
      "source": [
        "# Training function for the LSTM model\n",
        "def train(model, epochs, optimizer, criterion):\n",
        "    # Move the model to the specified device (GPU or CPU)\n",
        "    model = model.to(device=device)\n",
        "\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "\n",
        "    # Iterate over the specified number of epochs\n",
        "    for epoch in range(epochs):\n",
        "        # Initialize total loss variable\n",
        "        total_loss = 0.0\n",
        "\n",
        "        # Initialize correct predictions variable\n",
        "        correct_predictions = 0\n",
        "\n",
        "        # Initialize total samples variable\n",
        "        total_samples = 0\n",
        "\n",
        "        # Iterate over the batches in the training loader\n",
        "        for i, (data, targets) in enumerate(train_loader):\n",
        "            # Zero the gradients to prevent accumulation from previous iterations\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Move the input data and targets to the specified device (GPU or CPU)\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "            # Initialize the hidden state for the LSTM\n",
        "            hidden = model.init_hidden(data.size(0))\n",
        "\n",
        "            # Forward pass: obtain model predictions and hidden states\n",
        "            outputs, hidden = model(data, hidden)\n",
        "\n",
        "            # Calculate the loss using the specified criterion\n",
        "            # The criterion is a measure of the difference between model predictions and actual targets\n",
        "            loss = criterion(outputs.view(-1, len(vocab)), targets.view(-1))\n",
        "            # Accumulate the loss item to the total loss variable\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Extract the predicted labels with maximum probability along dimension 2\n",
        "            _, predicted = torch.max(outputs, 2)\n",
        "\n",
        "            # Update correct predictions count based on matching predictions with targets\n",
        "            correct_predictions += (predicted == targets).sum().item()\n",
        "\n",
        "            # Update total samples count by adding the number of elements in targets\n",
        "            total_samples += targets.numel()\n",
        "\n",
        "            # Backward pass: compute gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Optimizer step: update model parameters\n",
        "            optimizer.step()\n",
        "\n",
        "        # Calculate and print the average loss and accuracy for the current epoch\n",
        "        # The loss is averaged over all batches in the training set\n",
        "        loss = total_loss / len(train_loader)\n",
        "\n",
        "        # The accuracy is the ratio of correctly predicted samples to total samples\n",
        "        accuracy = correct_predictions / total_samples\n",
        "\n",
        "        # Print epoch, loss, and accuracy\n",
        "        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss:.4f}, Accuracy: {accuracy * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_9jV8e0EGX1W",
      "metadata": {
        "id": "_9jV8e0EGX1W"
      },
      "source": [
        "Next code is calling the train function to train a given model using the specified loss function, learning rate, and number of epochs. Here's a step-by-step summary:\n",
        "\n",
        "1. Define Loss Function:\n",
        "  - `loss_function = nn.CrossEntropyLoss()`: Creates an instance of the CrossEntropyLoss, which is commonly used for classification tasks.\n",
        "\n",
        "2. Set Learning Rate and Number of Epochs:\n",
        "  - `lr = 0.001`: Sets the learning rate for the Adam optimizer to 0.001.\n",
        "  - `epochs = 10:` Specifies the number of training epochs.\n",
        "\n",
        "3. Initialize Adam Optimizer:\n",
        "  - `optimiser = optim.Adam(model.parameters(), lr=lr)`: Creates an Adam optimizer for the model's parameters with the specified learning rate.\n",
        "\n",
        "4. Call the Training Function:\n",
        "  - `train(model, epochs, optimiser, loss_function)`: Calls the `train` function to train the model. It passes the model, number of epochs, optimizer, and loss function as arguments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa9c84ce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa9c84ce",
        "outputId": "afa73e74-393a-44a2-f99f-7e6006d8d21d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/30], Loss: 6.7737, Accuracy: 11.37%\n",
            "Epoch [2/30], Loss: 6.1135, Accuracy: 15.91%\n",
            "Epoch [3/30], Loss: 5.8485, Accuracy: 17.94%\n",
            "Epoch [4/30], Loss: 5.6813, Accuracy: 19.02%\n",
            "Epoch [5/30], Loss: 5.5551, Accuracy: 19.81%\n",
            "Epoch [6/30], Loss: 5.4519, Accuracy: 20.49%\n",
            "Epoch [7/30], Loss: 5.3626, Accuracy: 21.03%\n",
            "Epoch [8/30], Loss: 5.2840, Accuracy: 21.50%\n",
            "Epoch [9/30], Loss: 5.2135, Accuracy: 21.89%\n",
            "Epoch [10/30], Loss: 5.1484, Accuracy: 22.25%\n",
            "Epoch [11/30], Loss: 5.0883, Accuracy: 22.58%\n",
            "Epoch [12/30], Loss: 5.0325, Accuracy: 22.92%\n",
            "Epoch [13/30], Loss: 4.9799, Accuracy: 23.21%\n",
            "Epoch [14/30], Loss: 4.9300, Accuracy: 23.52%\n",
            "Epoch [15/30], Loss: 4.8833, Accuracy: 23.81%\n",
            "Epoch [16/30], Loss: 4.8387, Accuracy: 24.09%\n",
            "Epoch [17/30], Loss: 4.7964, Accuracy: 24.35%\n",
            "Epoch [18/30], Loss: 4.7559, Accuracy: 24.62%\n",
            "Epoch [19/30], Loss: 4.7172, Accuracy: 24.88%\n",
            "Epoch [20/30], Loss: 4.6804, Accuracy: 25.12%\n",
            "Epoch [21/30], Loss: 4.6446, Accuracy: 25.35%\n",
            "Epoch [22/30], Loss: 4.6106, Accuracy: 25.60%\n",
            "Epoch [23/30], Loss: 4.5777, Accuracy: 25.83%\n",
            "Epoch [24/30], Loss: 4.5463, Accuracy: 26.05%\n",
            "Epoch [25/30], Loss: 4.5160, Accuracy: 26.28%\n",
            "Epoch [26/30], Loss: 4.4872, Accuracy: 26.50%\n",
            "Epoch [27/30], Loss: 4.4588, Accuracy: 26.70%\n",
            "Epoch [28/30], Loss: 4.4318, Accuracy: 26.91%\n",
            "Epoch [29/30], Loss: 4.4059, Accuracy: 27.11%\n",
            "Epoch [30/30], Loss: 4.3808, Accuracy: 27.33%\n"
          ]
        }
      ],
      "source": [
        "# Define the loss function\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# Set the learning rate for the optimizer\n",
        "lr = 0.001\n",
        "\n",
        "# Set the number of training epochs\n",
        "epochs = 30\n",
        "\n",
        "# Initialize the optimizer (Adam) with the model parameters and specified learning rate\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# Call the training function with the model, number of epochs, optimizer, and loss function\n",
        "train(model, epochs, optimizer, loss_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TFbtdUDXHByQ",
      "metadata": {
        "id": "TFbtdUDXHByQ"
      },
      "source": [
        "Next code defines a function for text generation using a trained model. Here's a step-by-step summary:\n",
        "\n",
        "1. Define Text Generation Function:\n",
        "  - `def generate_text(model, start_text, num_words, temperature=1.0):`: Defines a function for generating text given a trained model, a starting text sequence (`start_text`), and the number of words to generate (`num_words`). The optional parameter `temperature` controls the level of randomness in the generation.\n",
        "\n",
        "2. Set Model to Evaluation Mode:\n",
        "  - `model.eval()`: Sets the model to evaluation mode, indicating that no gradients need to be computed during generation.\n",
        "\n",
        "3. Tokenize the Starting Text and Initialize Hidden State:\n",
        "  - `words = tokeniser(start_text)`: Tokenizes the starting text using the provided tokenizer.\n",
        "  - `hidden = model.init_hidden(1)`: Initializes the hidden state for the LSTM model. The batch size is set to 1 for text generation.\n",
        "\n",
        "4. Generate Text Loop:\n",
        "  - `with torch.no_grad():`: Ensures that no gradients are calculated during the generation loop.\n",
        "  - `for _ in range(num_words):`: Iterates over the specified number of words to generate.\n",
        "\n",
        "5. Prepare Input for the Model:\n",
        "  - `x = torch.tensor([[vocab[word] for word in words[-seq_length:]]], dtype=torch.long, device=device)`: Creates a tensor representing the input sequence for the model. It includes the last `seq_length` words from the generated text.\n",
        "\n",
        "6. Forward Pass Through the Model:\n",
        "  - `y_pred, hidden = model(x, hidden)`: Performs a forward pass through the model to get predictions for the next word.\n",
        "\n",
        "7. Sample the Next Word:\n",
        "  - `last_word_logits = y_pred[0][-1]`: Extracts the logits for the last word in the sequence.\n",
        "  - `p = F.softmax(last_word_logits / temperature, dim=0).cpu().numpy()`: Applies a softmax function with temperature to the logits, creating a probability distribution.\n",
        "  - `word_index = np.random.choice(len(last_word_logits), p=p)`: Samples the next word index based on the probability distribution.\n",
        "\n",
        "8. Append the Predicted Word to the Sequence:\n",
        "  - `words.append(vocab.lookup_token(word_index))`: Appends the predicted word to the list of generated words.\n",
        "\n",
        "9. Return the Generated Text:\n",
        "  - `return ' '.join(words)`: Joins the generated words into a text string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "H4fata_-MIT9",
      "metadata": {
        "id": "H4fata_-MIT9"
      },
      "outputs": [],
      "source": [
        "# Function to generate text using the trained LSTM model\n",
        "def generate_text(model, start_text, num_words, temperature=1.0):\n",
        "    '''\n",
        "    Generate text using the trained model.\n",
        "\n",
        "    Parameters:\n",
        "        model (LSTMModel): The trained LSTM model.\n",
        "        start_text (str): The starting text or seed.\n",
        "        num_words (int): The total number of words to generate.\n",
        "        temperature (float): Controls the randomness of the generated text.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated text.\n",
        "    '''\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize the starting text\n",
        "    words = tokeniser(start_text)\n",
        "\n",
        "    # Initialize the hidden state for the LSTM\n",
        "    hidden = model.init_hidden(1)\n",
        "\n",
        "    # Disable gradient computation for generation\n",
        "    with torch.no_grad():\n",
        "        # Generate the specified number of words\n",
        "        for _ in range(num_words):\n",
        "            # Prepare the input tensor for the model\n",
        "            x = torch.tensor([[vocab[word] for word in words[-seq_length:]]], dtype=torch.long, device=device)\n",
        "\n",
        "            # Forward pass: obtain model predictions and update hidden state\n",
        "            y_pred, hidden = model(x, hidden)\n",
        "\n",
        "            # Extract the logits for the last predicted word\n",
        "            last_word_logits = y_pred[0][-1]\n",
        "\n",
        "            # Calculate the softmax probabilities for the last predicted word's logits\n",
        "            p = F.softmax(last_word_logits / temperature, dim=0).cpu().numpy()\n",
        "\n",
        "            # Sample a word index based on the softmax distribution\n",
        "            word_index = np.random.choice(len(last_word_logits), p=p)\n",
        "\n",
        "            # Append the sampled word to the generated text\n",
        "            words.append(vocab.lookup_token(word_index))\n",
        "\n",
        "    # Combine the generated words into a coherent text\n",
        "    return ' '.join(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FU7s-E_iH7VR",
      "metadata": {
        "id": "FU7s-E_iH7VR"
      },
      "source": [
        "Finally code prints the result of generating text using the generate_text function with a pre-trained model. Here's a summary:\n",
        "\n",
        "- Generate Text:\n",
        "\n",
        "  - `generate_text(model, start_text=\"I like\", num_words=100)`: Calls the `generate_text` function with the following parameters:\n",
        "  - `model`: The pre-trained LSTM model.\n",
        "  - `start_text=\"I like\"`: The starting text for text generation.\n",
        "  - `num_words=100`: The desired number of words to generate.\n",
        "\n",
        "- Print the Generated Text:\n",
        "  - `print(...)`: Prints the result of the text generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2884543",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2884543",
        "outputId": "0fe29806-b86d-4591-f7c7-69a29b953b43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "i like a way it ’ s kubla khan had an expression of <unk> sprites . the song received it highly successful , if an ideal husband can engage in a booty form , <unk> , a book and her carriage begins gabbar and w habits in 1796 . = = = other campaign = = = = the song topped with fincher describing it as a nomadic school on developing a literal highlands and snake . jim kumar did not the threat of thermal home , bringing the use of solving public numerous glitches . the department of unosom z responded\n"
          ]
        }
      ],
      "source": [
        "# Generate and print text using the trained model\n",
        "print(generate_text(model, start_text=\"I like\", num_words=100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QX19SalxzE7A",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QX19SalxzE7A",
        "outputId": "befcfd81-d22e-48b6-ed11-03bb023b3211"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "when i was working funding for the tour . the world bury = = the major jain authority ( mosley ) is its fifth case internationals in the traditional post @-@ most prominent species of richmond ( <unk> <unk> ) decided to replace the main elite into its specific committee of <unk> , but slightly better rubber . . there were left out before several hundred thousand @ , @ 000 people , including a total of a multi @-@ growth ranger ( hymenium ) and <unk> <unk> . . if <unk> the eucalypts . ali dull down <unk> @-@ like hay or\n"
          ]
        }
      ],
      "source": [
        "# Test model with start text: When I was\n",
        "print(generate_text(model, start_text=\"When I was\", num_words=100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0YmnvQizNt0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0YmnvQizNt0",
        "outputId": "15b541e6-1089-4ede-f9e5-5220c8f24081"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "did you know . during the rothschild past , garcía márquez spoke of belief that she is credited as a crowd of formula one . i wanted to leave the world in the romantic , and considers him to put a bit victim bunch of zhou ' s comedy show ' s score . leaving pc gamer find the soviets . however , in 1922 , the region was educated in 1879 by three television series 1000 . <unk> , considered him the full @-@ colour in june 2007 , including one that would allow split by the rope fighting the length of\n"
          ]
        }
      ],
      "source": [
        "# Test model with start text: Did you know\n",
        "print(generate_text(model, start_text=\"Did you know\", num_words=100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jkNSAUKjzaNt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkNSAUKjzaNt",
        "outputId": "d07f7e54-e7b4-411e-a6ea-73cc6cc654ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "how about listeners of someone advises that ' explodes or mixed ] . alongside these performance it sparrow was the city ' s equaliser against thermonuclear water to intersect devin by peter milligan , mavis from the two spot during world war i , the british columbia , nebraska , walls to 1884 which also shows his total of four hundred days . manufactured by juan percival described the academy ball into a <unk> and abnormally crime honors , as he will <unk> his nature . the specific epithet galericulata is derived from the milky way . as a result , the\n"
          ]
        }
      ],
      "source": [
        "# Test model with start text; How about\n",
        "print(generate_text(model, start_text=\"How about\", num_words=100))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JjJ-soejO_dX",
      "metadata": {
        "id": "JjJ-soejO_dX"
      },
      "source": [
        "# **CONCLUSIONS**\n",
        "\n",
        "**Understanding of LSTM Networks:**\n",
        "\n",
        "The activity provided a valuable opportunity to gain a fundamental understanding of Long Short-Term Memory (LSTM) networks. By working with PyTorch and WikiText-2 data, we explored the inner workings of LSTMs and their application in sequence data processing.\n",
        "\n",
        "**Hands-on Experience with Sequence Data Processing:**\n",
        "\n",
        "Through the practical implementation of a simple LSTM text generator, we acquired hands-on experience in processing sequence data. This experience is crucial in grasping the challenges and nuances associated with working on sequential data, such as language.\n",
        "\n",
        "**Text Generation in PyTorch:**\n",
        "\n",
        "The activity focused on text generation using PyTorch, a popular deep learning framework. This hands-on approach not only reinforced our understanding of LSTMs but also provided insights into the practical aspects of implementing text generation models.\n",
        "\n",
        "**Realistic Expectations for Model Performance:**\n",
        "\n",
        "The activity emphasized the limitations of the model due to its simplicity, the amount of data, and computational resources. It was made clear that the generated text should not be expected to replace advanced models like ChatGPT. This realistic expectation is crucial for understanding the practical boundaries of the implemented model.\n",
        "\n",
        "**Academic Purpose and Learning Objectives:**\n",
        "\n",
        "The primary purpose of the activity was academic—to deepen our comprehension of text generation using Recurrent Neural Networks (RNNs), specifically LSTMs. The objectives were not centered around creating a highly performant model but rather around learning key concepts and gaining practical skills.\n",
        "\n",
        "**Code Comprehension and Documentation Skills:**\n",
        "\n",
        "An additional focus of the activity was on enhancing code comprehension and documentation skills. This was achieved through the task of commenting on provided starter code. This skill is essential in collaborative coding environments and contributes to the overall understanding of the implemented solution.\n",
        "\n",
        "The activity successfully achieved its educational goals by providing a hands-on experience with LSTM networks, text generation in PyTorch, and reinforcing code comprehension and documentation skills. The realistic expectations set for the model's performance add a practical perspective to the academic exercise."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a98b713",
      "metadata": {
        "id": "4a98b713"
      },
      "source": [
        "### Long Short-Term Memory VS Classification Models\n",
        "\n",
        "LSTMs are designed to remember information over long sequences, aiding their objective of predicting words based on the probability of which word should follow the ones previously stated or generated. In contrast, a classifier's objective is to assign a label or score to the entire input. For example, in sentiment analysis, each word is represented by a vector (assuming use of Word2Vec or GloVe). This vector then feeds into an evaluation function, such as a neural network, logistic regression, or an LSTM—the latter being the most complex process. An LSTM processes the input word by word sequentially. The main difference is that our model assigns sequential value based on the previous tokens/words provided, whereas in a classification model, the focus is on a final score—a classification score.\n",
        "\n",
        "Another significant difference between these two models lies in how they calculate loss. The LSTM calculates its cross-entropy loss at each step, with each step corresponding to each token/word. It does this by comparing the predicted next word against the actual word in the training data, and the loss is accumulated over the sequence to adjust the model during training. On the other hand, in classification, the cross-entropy loss is calculated by comparing the predicted overall sentiment score of a sentence against the actual sentiment score, using sentiment analysis as an example."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ddb4500",
      "metadata": {
        "id": "1ddb4500"
      },
      "source": [
        "### Model Improvements\n",
        "\n",
        "**LSTM Architecture Tweaks:** You might want to add dropout layers to your LSTM to avoid overfitting, especially if you're dealing with a complex dataset. This basically gives the model a better chance to generalize by randomly turning off some neurons during training. Also, playing around with the number of LSTM layers can help. More layers can capture complex patterns, but they also make your model more complex and increase the risk of overfitting.\n",
        "\n",
        "**Hyperparameter Fine-Tuning:** Adjusting the learning rate and batch size could really make a difference. A learning rate scheduler could be a game-changer, as it changes the learning rate during training to improve performance. Changing the batch size could also help in finding the sweet spot between how fast and how well your model trains. Plus, tweaking the size of your word embeddings might be worth a shot. Bigger embeddings could mean better understanding of word meanings, but they also make your model heavier.\n",
        "\n",
        "**Enhanced Training Strategies:** Implementing gradient clipping can stabilize your training, particularly for a deep LSTM network. It essentially keeps the gradients in check to avoid extreme updates. Also, using teacher forcing during training could speed up your model’s learning. It means using the actual next word from your dataset during training, instead of the word your model predicts, helping it learn faster."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ea540ce",
      "metadata": {
        "id": "6ea540ce"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}